# langchain_test.py  (VQA ë‹¨ë… í…ŒìŠ¤íŠ¸ìš©)

import os
import logging

import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

# =========================
# ë¡œê¹… ì„¤ì •
# =========================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =========================
# ì„¤ì •ê°’
# =========================
MODEL_NAME = "Rfy23/qwenvl-7B-medical-ko-zh"
IMAGE_URL = "BackEnd/testimage.jpg"  # ì‹¤í–‰ ìœ„ì¹˜ì— ë”°ë¼ ìƒëŒ€ê²½ë¡œê°€ ê¼¬ì¼ ìˆ˜ ìˆì–´ ì•„ë˜ì—ì„œ absë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
FIXED_QUESTION = "è¿™å¼ å¤„æ–¹ä¸Šå†™äº†ä»€ä¹ˆï¼Ÿ å°¤å…¶æ˜¯è¯å“ã€æœç”¨æ¬¡æ•°ç­‰ï¼Œè¯·å‡†ç¡®å…¨éƒ¨å‘Šè¯‰æˆ‘ã€‚"

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

QWEN_MODEL = None
QWEN_PROCESSOR = None


def load_qwen_components(model_name: str, device: str):
    """Qwen2VL ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ì „ì—­ìœ¼ë¡œ 1íšŒ ë¡œë“œ"""
    global QWEN_MODEL, QWEN_PROCESSOR

    if QWEN_MODEL is not None and QWEN_PROCESSOR is not None:
        logger.info("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    logger.info(f"ğŸš€ VQA ëª¨ë¸ '{model_name}' ë¡œë“œ ì‹œì‘ (Device: {device})")
    torch_dtype = torch.float16 if device == "cuda" else torch.float32
    device_map = "auto" if device == "cuda" else None

    try:
        print("ëª¨ë¸ ë¡œë“œ ì¤‘...")
        QWEN_MODEL = Qwen2VLForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch_dtype,
            device_map=device_map
        )
        QWEN_MODEL.eval()

        QWEN_PROCESSOR = AutoProcessor.from_pretrained(model_name)
        print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        logger.info("âœ… ëª¨ë¸/í”„ë¡œì„¸ì„œ ë¡œë“œ ì™„ë£Œ")

    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        raise


def vqa_model(image_path: str, question: str = FIXED_QUESTION, max_new_tokens: int = 128) -> str:
    """ì´ë¯¸ì§€ ê²½ë¡œ + ì§ˆë¬¸ -> Qwen2VLë¡œ ë‹µë³€ ìƒì„±"""
    global QWEN_MODEL, QWEN_PROCESSOR

    if QWEN_MODEL is None or QWEN_PROCESSOR is None:
        return "ì˜¤ë¥˜: ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_qwen_components()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”."

    if not os.path.exists(image_path):
        return f"ì˜¤ë¥˜: ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {image_path}"

    try:
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image_path},
                    {"type": "text", "text": f"<image>\n{question}"}
                ],
            }
        ]

        logger.info(f"ğŸ–¼ï¸ VQA ì¶”ë¡  ì‹œì‘ - Image: {image_path}")
        print("ì…ë ¥ í…ì„œ ì¤€ë¹„ ì¤‘...")

        text_input = QWEN_PROCESSOR.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        image_inputs, _ = process_vision_info(messages)

        inputs = QWEN_PROCESSOR(
            text=[text_input],
            images=image_inputs,
            padding=True,
            return_tensors="pt"
        ).to(DEVICE)

        with torch.no_grad():
            generated_ids = QWEN_MODEL.generate(**inputs, max_new_tokens=max_new_tokens)

        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]

        output_text = QWEN_PROCESSOR.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )

        logger.info("âœ… VQA ì¶”ë¡  ì™„ë£Œ")
        return output_text[0]

    except Exception as e:
        return f"VQA ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


if __name__ == "__main__":
    # ê²½ë¡œ í™•ì‹¤íˆ(ìƒëŒ€ê²½ë¡œ ì´ìŠˆ ë°©ì§€)
    IMAGE_URL_ABS = os.path.abspath(IMAGE_URL)
    print("ì´ë¯¸ì§€ ê²½ë¡œ:", IMAGE_URL_ABS)
    print("íŒŒì¼ ì¡´ì¬?:", os.path.exists(IMAGE_URL_ABS))
    print("DEVICE:", DEVICE)

    # 1íšŒ ë¡œë“œ í›„ ì¶”ë¡ 
    load_qwen_components(MODEL_NAME, DEVICE)
    result = vqa_model(IMAGE_URL_ABS, FIXED_QUESTION)

    print("\n===== VQA ê²°ê³¼ =====")
    print(result)