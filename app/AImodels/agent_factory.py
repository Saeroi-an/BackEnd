# app/AImodels/agent_factory.py
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain.agents import AgentExecutor, create_react_agent
from langchain.memory import ConversationBufferMemory
from langchain import hub  # ðŸ‘ˆ ì¶”ê°€
import torch
import os
import logging
from app.AImodels.tools import ALL_TOOLS

logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
REPO_ID = os.getenv('LLM_REPO_ID', 'google/flan-t5-large')

logger.info(f"ðŸ” LLM_REPO_ID: {REPO_ID}")

# ì „ì—­ ë³€ìˆ˜
huggingfacehub = None
initial_agent = None
GLOBAL_TOOLS = ALL_TOOLS

def initialize_global_agent():
    """ì „ì—­ LLMê³¼ Toolì„ ì´ˆê¸°í™” (ë¡œì»¬ ëª¨ë¸)"""
    global huggingfacehub, GLOBAL_TOOLS, initial_agent
    
    try:
        logger.info("ðŸš€ Initializing Global LLM and Tools (Local Model)...")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
        device = 0 if torch.cuda.is_available() else -1
        logger.info(f"ðŸ–¥ï¸ Using device: {'GPU' if device == 0 else 'CPU'}")
        
        # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(REPO_ID)
        model = AutoModelForSeq2SeqLM.from_pretrained(
            REPO_ID,
            torch_dtype=torch.float16 if device == 0 else torch.float32,
            device_map="auto" if device == 0 else None
        )
        
        # Pipeline ìƒì„±
        pipe = pipeline(
            "text2text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=512,
            temperature=0.1
        )
        
        # LangChain LLMìœ¼ë¡œ ëž˜í•‘
        huggingfacehub = HuggingFacePipeline(pipeline=pipe)
        
        logger.info(f"âœ… Tools loaded: {[tool.name for tool in GLOBAL_TOOLS]}")
        
        initial_agent = True
        
        logger.info(f"âœ… LLM initialized (Local): {REPO_ID}")
        logger.info(f"âœ… Total tools: {len(GLOBAL_TOOLS)}")
        
    except Exception as e:
        logger.error(f"âŒ LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        huggingfacehub = None
        initial_agent = False
        raise

def create_agent_executor(memory_instance: ConversationBufferMemory):
    """ì„¸ì…˜ë³„ Agent Executor ìƒì„±"""
    if not huggingfacehub or not initial_agent:
        raise RuntimeError("LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    logger.info("ðŸ”§ Creating Agent Executor with memory...")
    
    # LangChain Hubì—ì„œ ê³µì‹ ReAct í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
    prompt = hub.pull("hwchase17/react")
    
    agent = create_react_agent(
        llm=huggingfacehub,
        tools=GLOBAL_TOOLS,
        prompt=prompt
    )
    
    agent_executor = AgentExecutor(
        agent=agent,
        tools=GLOBAL_TOOLS,
        memory=memory_instance,
        verbose=True,
        handle_parsing_errors=True,
        max_iterations=5
    )
    
    logger.info("âœ… Agent Executor created successfully")
    
    return agent_executor

SESSION_MEMORY_CACHE = {}

def cleanup_old_sessions(max_sessions: int = 1000):
    """ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬"""
    if len(SESSION_MEMORY_CACHE) > max_sessions:
        keys_to_delete = list(SESSION_MEMORY_CACHE.keys())[:len(SESSION_MEMORY_CACHE) // 2]
        for key in keys_to_delete:
            del SESSION_MEMORY_CACHE[key]
        logger.info(f"ðŸ§¹ Cleaned up {len(keys_to_delete)} old sessions")

GLOBAL_AGENT_EXECUTOR = None