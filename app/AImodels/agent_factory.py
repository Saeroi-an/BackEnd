# app/AImodels/agent_factory.py
import os
import logging
from supabase import Client

# LangChain ìµœì‹  ë²„ì „(1.x)ì—ì„œëŠ” ê¸°ì¡´ì˜ ì—ì´ì „íŠ¸ êµ¬í˜„(AgentExecutor, ReAct agent ë“±)ì´
# 'langchain' ë³¸ íŒ¨í‚¤ì§€ì—ì„œ ë¶„ë¦¬ë˜ì–´ 'langchain-classic' íŒ¨í‚¤ì§€ë¡œ ì´ë™í•œ ê²½ìš°ê°€ ë§ŽìŒ.
# ê¸°ì¡´ ì½”ë“œ(ë ˆê±°ì‹œ ReAct ì—ì´ì „íŠ¸)ë¥¼ ìœ ì§€í•˜ë ¤ë©´ classicì—ì„œ ê°€ì ¸ì˜¤ëŠ” ê²Œ ì•ˆì •ì .
from langchain_classic.agents import AgentExecutor, create_openai_tools_agent
# â†‘ AgentExecutor: "Agent(ì¶”ë¡  ë¡œì§) + Tools(ë„êµ¬)"ë¥¼ ë¬¶ì–´ì„œ ì‹¤í–‰(invoke)í•  ìˆ˜ ìžˆê²Œ í•´ì£¼ëŠ” ì‹¤í–‰ê¸°
from langchain_openai import ChatOpenAI
from langchain.tools import tool
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from app.AImodels.tools import ALL_TOOLS
from app.core.config import settings


logger = logging.getLogger(__name__)


# ì „ì—­ ë³€ìˆ˜
GLOBAL_TOOLS = ALL_TOOLS
GLOBAL_LLM = None

def initialize_global_agent():
    """ì „ì—­ LLMê³¼ Toolì„ ì´ˆê¸°í™” (ë¡œì»¬ ëª¨ë¸)"""
    global GLOBAL_TOOLS, GLOBAL_LLM
    
    try:
        logger.info("ðŸš€ OpenAI ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
        
        # Initialize the language model with specific parameters
        llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            api_key=settings.OPENAI_API_KEY,
            temperature=0.1,  # Low temperature for consistent reasoning
            max_tokens=2000,
            timeout=30
        )
        
        logger.info(f"âœ… global tools ì¶œë ¥ í™•ì¸: {GLOBAL_TOOLS}")
        logger.info(f"âœ… Tools loaded: {[tool.name for tool in GLOBAL_TOOLS]}")
        
        
        GLOBAL_LLM = llm
        
        logger.info(f"âœ… Total tools: {len(GLOBAL_TOOLS)}")
        
    except Exception as e:
        logger.error(f"âŒ OpenAI ë¶ˆëŸ¬ì˜¤ê¸° ì˜¤ë¥˜: {e}")

        raise

# ì´ì œ Supabase ì§ì ‘ ì ‘ê·¼
def create_agent_executor(supabase: Client, user_id: str):
    """ì„¸ì…˜ë³„ Agent Executor ìƒì„± (Memory ì—†ì´ Supabase ì§ì ‘ ì‚¬ìš©)"""
    # Import at function level to avoid circular dependency
    from app.services.chat_service import load_chat_history_from_db
    
    global GLOBAL_LLM, GLOBAL_TOOLS
    
    if GLOBAL_LLM is None:
        logger.error("âŒ LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        raise ValueError("LLM is not initialized.")
    
    logger.info(f"ðŸ”§ Creating Agent Executor for user: {user_id}")
    
    
    chat_history = load_chat_history_from_db(supabase, user_id)
    chat_history_text = chat_history[0] if chat_history else ""
 
    # Create optimized prompt template # âœ… check
    react_prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a helpful medical assistant for Chinese patients in Korea.

Your capabilities:
- Analyze prescription images when users upload them
- Search for Korean drug information
- Answer medical questions in Chinese

Guidelines:
- Always respond in Chinese (ä¸­æ–‡)
- Be clear and concise
- If you need to analyze a prescription image, use the available tools
- If asked about drug information, search using the drug API tool
- Provide helpful medical guidance while being cautious about medical advice

Remember: You're helping Chinese patients understand Korean prescriptions and medical information."""),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}")
    ])

#     react_prompt = PromptTemplate.from_template("""You are a helpful medical assistant. Answer questions based on the tools available and conversation history.

# Available tools:
# {tools}

# Tool Names: {tool_names}

# Guidelines:
# - If the question contains "prescription_id: [number]", use VL_Model_Image_Analyzer with that number as input 
# - For drug information questions, use Public_Data_API_Searcher
# - Otherwise, answer based on your knowledge

# Use this format:
# Question: the input question
# Thought: think about what to do
# Action: the tool to use (one of [{tool_names}]) OR say "No tool needed"
# Action Input: the input for the tool (if using a tool)
# Observation: the tool's response
# ... (repeat Thought/Action/Observation if needed)
# Thought: I now know the final answer
# Final Answer: the complete answer to the question

# Begin!

# Previous conversation:
# {chat_history}

# Question: {user_query}""")
    
    
    
    agent = create_openai_tools_agent(
        llm=GLOBAL_LLM,
        tools=GLOBAL_TOOLS,
        prompt=react_prompt
    )
    
    agent_executor = AgentExecutor(
        agent=agent,
        tools=GLOBAL_TOOLS,
        # memory=memory_instance, : memory íŒŒë¼ë¯¸í„° ì œê±°: ì±„íŒ… ê¸°ë¡ì´ ì´ë¯¸ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ë¨ & LangChain Memory ì‹œìŠ¤í…œ ë¶ˆí•„ìš”
        verbose=True,
        handle_parsing_errors=True,
        max_iterations=3 
    )
    
    logger.info("ReAct agent created successfully")
    
    # executor ê°ì²´ë§Œ ë°˜í™˜: invokeëŠ” chat_serviceì—ì„œ ì‹¤í–‰ & agent_factoryëŠ” ìƒì„±ë§Œ ë‹´ë‹¹
    # ai_response = agent_executor.invoke({"input": user_query})
    logger.info("âœ… Agent Executor created successfully")
    # logger.info(f"ëž­ì²´ì¸ì´ ìƒì„±í•œ ë‹µë³€: {ai_response}")
    
    # return ai_response # string
    return agent_executor

SESSION_MEMORY_CACHE = {}

def cleanup_old_sessions(max_sessions: int = 1000):
    """ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬"""
    if len(SESSION_MEMORY_CACHE) > max_sessions:
        keys_to_delete = list(SESSION_MEMORY_CACHE.keys())[:len(SESSION_MEMORY_CACHE) // 2]
        for key in keys_to_delete:
            del SESSION_MEMORY_CACHE[key]
        logger.info(f"ðŸ§¹ Cleaned up {len(keys_to_delete)} old sessions")

GLOBAL_AGENT_EXECUTOR = None