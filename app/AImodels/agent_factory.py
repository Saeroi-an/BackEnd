# app/AImodels/agent_factory.py
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain.agents import AgentExecutor, create_react_agent
from langchain.memory import ConversationBufferMemory
import torch
import os
import logging

from app.AImodels.tools import ALL_TOOLS

logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
REPO_ID = os.getenv('LLM_REPO_ID', 'google/flan-t5-large')

logger.info(f"ðŸ” LLM_REPO_ID: {REPO_ID}")

# ì „ì—­ ë³€ìˆ˜
huggingfacehub = None
initial_agent = None
GLOBAL_TOOLS = ALL_TOOLS


def initialize_global_agent():
    """ì „ì—­ LLMê³¼ Toolì„ ì´ˆê¸°í™” (ë¡œì»¬ ëª¨ë¸)"""
    global huggingfacehub, GLOBAL_TOOLS, initial_agent
    
    try:
        logger.info("ðŸš€ Initializing Global LLM and Tools (Local Model)...")
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
        device = 0 if torch.cuda.is_available() else -1
        logger.info(f"ðŸ–¥ï¸ Using device: {'GPU' if device == 0 else 'CPU'}")
        
        # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(REPO_ID)
        model = AutoModelForSeq2SeqLM.from_pretrained(
            REPO_ID,
            torch_dtype=torch.float16 if device == 0 else torch.float32,
            device_map="auto" if device == 0 else None
        )
        
        # Pipeline ìƒì„±
        pipe = pipeline(
            "text2text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=512,
            temperature=0.1,
            device=device
        )
        
        # LangChain LLMìœ¼ë¡œ ëž˜í•‘
        huggingfacehub = HuggingFacePipeline(pipeline=pipe)
        
        logger.info(f"âœ… Tools loaded: {[tool.name for tool in GLOBAL_TOOLS]}")
        
        initial_agent = True
        
        logger.info(f"âœ… LLM initialized (Local): {REPO_ID}")
        logger.info(f"âœ… Total tools: {len(GLOBAL_TOOLS)}")
        
    except Exception as e:
        logger.error(f"âŒ LLM ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        huggingfacehub = None
        initial_agent = False
        raise


def create_agent_executor(memory_instance: ConversationBufferMemory):
    """ì„¸ì…˜ë³„ Agent Executor ìƒì„±"""
    if not huggingfacehub or not initial_agent:
        raise RuntimeError("LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    logger.info("ðŸ”§ Creating Agent Executor with memory...")
    
    from langchain.prompts import PromptTemplate
    
    template = """Answer the following questions as best you can. You have access to the following tools:

{tools}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

Question: {input}
Thought:{agent_scratchpad}"""
    
    prompt = PromptTemplate.from_template(template)
    
    agent = create_react_agent(
        llm=huggingfacehub,
        tools=GLOBAL_TOOLS,
        prompt=prompt
    )
    
    agent_executor = AgentExecutor(
        agent=agent,
        tools=GLOBAL_TOOLS,
        memory=memory_instance,
        verbose=True,
        handle_parsing_errors=True,
        max_iterations=5
    )
    
    logger.info("âœ… Agent Executor created successfully")
    
    return agent_executor


SESSION_MEMORY_CACHE = {}

def cleanup_old_sessions(max_sessions: int = 1000):
    """ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬"""
    if len(SESSION_MEMORY_CACHE) > max_sessions:
        keys_to_delete = list(SESSION_MEMORY_CACHE.keys())[:len(SESSION_MEMORY_CACHE) // 2]
        for key in keys_to_delete:
            del SESSION_MEMORY_CACHE[key]
        logger.info(f"ðŸ§¹ Cleaned up {len(keys_to_delete)} old sessions")

GLOBAL_AGENT_EXECUTOR = None