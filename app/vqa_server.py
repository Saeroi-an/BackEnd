import gc
import os
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info # ì‹¤ì œ ì‚¬ìš© ì‹œ ì„í¬íŠ¸ í•„ìš”

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

# ------------------------------
# 1. FastAPI ì„¤ì •
# ------------------------------
app = FastAPI()

# Pydantic ëª¨ë¸: ì…ë ¥ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ (LangChain JS/TS ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜í•´ì•¼ í•¨)
class VQAInput(BaseModel):
    image_path: str
    question: str
    prescription_id: int

# ------------------------------
# 2. ëª¨ë¸ ì „ì—­ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
# ------------------------------
model = None
processor = None
device = "cuda" if torch.cuda.is_available() else "cpu"

@app.on_event("startup")
async def load_vqa_model():
    """ì„œë²„ ì‹œì‘ ì‹œ Qwen2VL ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ"""
    global model, processor
    model_name = "Rfy23/qwenvl-7B-medical-ko-zh"
    
    print("ğŸš€ Qwen2VL ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
    try:
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None
        ).eval()
        processor = AutoProcessor.from_pretrained(model_name)
        print("âœ… ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì„œë²„ë¥¼ ì¢…ë£Œí•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# ------------------------------
# 3. VQA ì¶”ë¡  API ì—”ë“œí¬ì¸íŠ¸
# ------------------------------
@app.post("/api/vqa_inference")
async def vqa_inference_endpoint(input_data: VQAInput):
    if model is None or processor is None:
        raise HTTPException(status_code=503, detail="VQA ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # LangChain Toolì—ì„œ ë°›ì€ ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    image_url = input_data.image_path
    question = input_data.question

    # ìš”ì²­ ë¡œê¹…
    print(f"ğŸ“¥ ë°›ì€ ì§ˆë¬¸: {question}")
    
    # **ì—¬ê¸°ì— ì‚¬ìš©ìë‹˜ì˜ Qwen2VL ì¶”ë¡  ë¡œì§ì„ ë„£ìŠµë‹ˆë‹¤.**
    # (messages, processor.apply_chat_template, process_vision_info, model.generate ì½”ë“œ)
    
    # ì¶”ë¡  ë¡œì§ ì‹¤í–‰...
    try:
        # 2. ë©”ì‹œì§€ êµ¬ì„± (image_urlê³¼ question ì‚¬ìš©)
        messages = [
             {
                 "role": "user",
                 "content": [
                     {"type": "image", "image": image_url},
                     {"type": "text", "text": f"<image>\n{question}"}
                 ],
             }
        ]
        
        # 3. processorë¡œ ì…ë ¥ ì¤€ë¹„       
        print("ì…ë ¥ í…ì„œ ì¤€ë¹„ ì¤‘...")
        text_input = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, _ = process_vision_info(messages)

        inputs = processor(
            text=[text_input],
            images=image_inputs,
            # videos=video_inputs,
            padding=True,
            return_tensors="pt"
        ).to(device)
        print("ì…ë ¥ í…ì„œ ì¤€ë¹„ ì™„ë£Œ!")

        
        # 4. ì¶”ë¡ 
        print("ëª¨ë¸ ì¶”ë¡  ì‹œì‘...")
        with torch.no_grad():
            generated_ids = model.generate(**inputs, max_new_tokens=280)

        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]

        print("ë””ì½”ë”© ì¤‘...")
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )

        print("ëª¨ë¸ ì¶œë ¥:")
        
        inference_result = f"ì²˜ë°© ID {input_data.prescription_id}ì— ëŒ€í•œ ì¶”ë¡  ì¶œë ¥: \n {output_text[0]}"
        
        # GPU ë©”ëª¨ë¦¬ ì¦‰ì‹œ ì •ë¦¬
        del generated_ids, generated_ids_trimmed, inputs, image_inputs
        torch.cuda.empty_cache()
        gc.collect()
        print(f"âœ… GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ (prescription_id: {input_data.prescription_id})")
        
        return {"prescription_id": input_data.prescription_id, "inference_result": inference_result}
        
    except Exception as e:
        print(f"ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"VQA ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ì„œë²„ ì‹¤í–‰ ëª…ë ¹ì–´: uvicorn main:app --reload --port 8000