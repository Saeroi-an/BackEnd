# app/api/vqa_inference.py
import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

router = APIRouter()

class VQAInput(BaseModel):
    image_path: str
    question: str
    prescription_id: int

# ì „ì—­ ë³€ìˆ˜
model = None
processor = None
device = "cuda" if torch.cuda.is_available() else "cpu"

def load_model_if_needed():
    """ì²« ìš”ì²­ ì‹œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤ (Lazy Loading)"""
    global model, processor
    if model is not None:
        return
    
    model_name = "Rfy23/qwen2vl-ko-zh"
    print("ğŸš€ Qwen2VL ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
    try:
        model = Qwen2VLForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None
        ).eval()
        processor = AutoProcessor.from_pretrained(model_name)
        print("âœ… ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ ì™„ë£Œ!")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
        raise

@router.post("/vqa_inference")
async def vqa_inference_endpoint(input_data: VQAInput):
    """VQA ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸: ì´ë¯¸ì§€ ê²½ë¡œì™€ ì§ˆë¬¸ì„ ë°›ì•„ Qwen2VL ëª¨ë¸ë¡œ ë¶„ì„"""
    load_model_if_needed()
    
    if model is None or processor is None:
        raise HTTPException(status_code=503, detail="VQA ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    image_url = input_data.image_path
    question = input_data.question
    
    try:
        # 1. ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image_url},
                    {"type": "text", "text": f"<image>\n{question}"}
                ],
            }
        ]
        
        # 2. ì…ë ¥ í…ì„œ ì¤€ë¹„
        print("ì…ë ¥ í…ì„œ ì¤€ë¹„ ì¤‘...")
        text_input = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, _ = process_vision_info(messages)
        inputs = processor(
            text=[text_input],
            images=image_inputs,
            padding=True,
            return_tensors="pt"
        ).to(device)
        
        # 3. ëª¨ë¸ ì¶”ë¡ 
        print("ëª¨ë¸ ì¶”ë¡  ì‹œì‘...")
        with torch.no_grad():
            generated_ids = model.generate(**inputs, max_new_tokens=280)
        
        # 4. ë””ì½”ë”©
        generated_ids_trimmed = [
            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        
        # 5. ê²°ê³¼ ë°˜í™˜
        inference_result = f"ì²˜ë°© ID {input_data.prescription_id}ì— ëŒ€í•œ ì¶”ë¡  ì¶œë ¥: \n {output_text[0]}"
        
        print(f"âœ… VQA ì¶”ë¡  ì™„ë£Œ - ì²˜ë°© ID: {input_data.prescription_id}")
        return {"prescription_id": input_data.prescription_id, "inference_result": inference_result}
        
    except Exception as e:
        print(f"âŒ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=f"VQA ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")